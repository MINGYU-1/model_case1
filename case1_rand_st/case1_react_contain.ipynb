{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13cc1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "par_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "os.chdir(par_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f768142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vae_earlystopping import EarlyStopping\n",
    "from model.m2_bce import BCEcVAE\n",
    "from model.m2_mse import MSEcVAE\n",
    "from loss.l2_bce import l2_bce\n",
    "from loss.l2_mse import l2_mse\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e98dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (352415590.py, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 92\u001b[1;36m\u001b[0m\n\u001b[1;33m    gamma_list = [0.1, 0.2, ,0.3,0.4,0.5, 0.6,0.7,0.8,0.9, 1.0]\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- train_and_evaluate 함수는 이전과 동일 (gamma_val 인자 사용) ---\n",
    "def train_and_evaluate(c_path, x_data, seed, gamma_val):\n",
    "    c_data = np.load(c_path)\n",
    "    x_train, x_temp, c_train, c_temp = train_test_split(x_data, c_data, random_state=seed, test_size=0.4)\n",
    "    x_val, x_test, c_val, c_test = train_test_split(x_temp, c_temp, random_state=seed, test_size=0.5)\n",
    "    \n",
    "    x_scaler, c_scaler = MinMaxScaler(), MinMaxScaler()\n",
    "    x_train = x_scaler.fit_transform(x_train)\n",
    "    c_train = c_scaler.fit_transform(c_train)\n",
    "    x_val, x_test = x_scaler.transform(x_val), x_scaler.transform(x_test)\n",
    "    c_val, c_test = c_scaler.transform(c_val), c_scaler.transform(c_test)\n",
    "    \n",
    "    t_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.FloatTensor(c_train)), batch_size=64, shuffle=True)\n",
    "    v_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.FloatTensor(c_val)), batch_size=64)\n",
    "    test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.FloatTensor(c_test)), batch_size=64)\n",
    "    \n",
    "    x_dim, c_dim = x_train.shape[1], c_train.shape[1]\n",
    "\n",
    "    # --- 1. BCE 모델 학습 ---\n",
    "    model_bce = BCEcVAE(x_dim, c_dim, z_dim=8).to(device)\n",
    "    opt_bce = optim.Adam(model_bce.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    es_bce = EarlyStopping(patience=40, min_delta=1e-9)\n",
    "    \n",
    "    for epoch in range(1, 801):\n",
    "        model_bce.train()\n",
    "        for x, c in t_loader:\n",
    "            x, c = x.to(device), c.to(device)\n",
    "            opt_bce.zero_grad()\n",
    "            logit, mu, logvar = model_bce(x, c)\n",
    "            loss = l2_bce(logit, x, mu, logvar, beta=1, gamma=gamma_val)['loss']\n",
    "            loss.backward(); opt_bce.step()\n",
    "        \n",
    "        model_bce.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for vx, vc in v_loader:\n",
    "                vx, vc = vx.to(device), vc.to(device)\n",
    "                vl, vm, vv = model_bce(vx, vc)\n",
    "                val_loss += l2_bce(vl, vx, vm, vv, beta=1, gamma=gamma_val)['loss'].item()\n",
    "        if es_bce(val_loss/len(v_loader), model_bce): break\n",
    "    es_bce.load_best_model(model_bce)\n",
    "\n",
    "    # --- 2. MSE 모델 학습 ---\n",
    "    model_mse = MSEcVAE(x_dim, c_dim, z_dim=8).to(device)\n",
    "    opt_mse = optim.Adam(model_mse.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    es_mse = EarlyStopping(patience=40, min_delta=1e-9)\n",
    "    \n",
    "    for epoch in range(1, 801):\n",
    "        model_mse.train()\n",
    "        for x, c in t_loader:\n",
    "            x, c = x.to(device), c.to(device)\n",
    "            opt_mse.zero_grad()\n",
    "            xh, mu, logvar = model_mse(x, c)\n",
    "            loss = l2_mse(xh, x, mu, logvar, alpha=1, gamma=gamma_val)['loss']\n",
    "            loss.backward(); opt_mse.step()\n",
    "            \n",
    "        model_mse.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for vx, vc in v_loader:\n",
    "                vx, vc = vx.to(device), vc.to(device)\n",
    "                xh, vm, vv = model_mse(vx, vc)\n",
    "                val_loss += l2_mse(xh, vx, vm, vv, alpha=1, gamma=gamma_val)['loss'].item()\n",
    "        if es_mse(val_loss/len(v_loader), model_mse): break\n",
    "    es_mse.load_best_model(model_mse)\n",
    "\n",
    "    # --- 3. 최종 평가 ---\n",
    "    model_bce.eval(); model_mse.eval()\n",
    "    all_bce_logits, all_mse_preds, all_true_scaled = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xt, ct in test_loader:\n",
    "            xt, ct = xt.to(device), ct.to(device)\n",
    "            b_logit, _, _ = model_bce(xt, ct)\n",
    "            m_pred, _, _ = model_mse(xt, ct)\n",
    "            all_bce_logits.append(b_logit.cpu().numpy())\n",
    "            all_mse_preds.append(m_pred.cpu().numpy())\n",
    "            all_true_scaled.append(xt.cpu().numpy())\n",
    "\n",
    "    bce_logits, mse_preds_scaled, x_true_scaled = np.vstack(all_bce_logits), np.vstack(all_mse_preds), np.vstack(all_true_scaled)\n",
    "    bce_prob = 1 / (1 + np.exp(-bce_logits)) \n",
    "    combined_scaled = mse_preds_scaled * bce_prob\n",
    "    final_pred_phys = x_scaler.inverse_transform(combined_scaled)\n",
    "    x_true_phys = x_scaler.inverse_transform(x_true_scaled)\n",
    "    \n",
    "    return float(r2_score(x_true_phys.flatten(), final_pred_phys.flatten()))\n",
    "\n",
    "# --- 메인 통합 실험 루프 ---\n",
    "x_data = np.load('./data/metal.npy')\n",
    "gamma_list = [0.1, 0.2,0.3,0.4,0.5, 0.6,0.7,0.8,0.9, 1.0]\n",
    "seeds = np.random.randint(1, 100, size=20)\n",
    "\n",
    "# 모든 결과를 담을 통합 딕셔너리\n",
    "all_gamma_results = {}\n",
    "\n",
    "for g in gamma_list:\n",
    "    g_key = f\"gamma_{str(g).replace('.', '_')}\"\n",
    "    all_gamma_results[g_key] = []\n",
    "    print(f\"\\n>>> Running Experiment for Gamma: {g}\")\n",
    "    \n",
    "    for i, seed in enumerate(seeds):\n",
    "        r2_val = train_and_evaluate('./data/pre_re_fin_1.npy', x_data, seed, g)\n",
    "        all_gamma_results[g_key].append(r2_val)\n",
    "        print(f\"   [Seed {seed}] R2: {r2_val:.4f}\")\n",
    "\n",
    "# 최종 통합 저장\n",
    "save_path = \"./Combined_Gamma_Results.json\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"metadata\": {\"seeds\": seeds.tolist(), \"gamma_tested\": gamma_list},\n",
    "        \"results\": all_gamma_results\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ 모든 실험 완료! 결과가 {save_path}에 통합 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
