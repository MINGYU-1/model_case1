{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "init_import",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 부모 디렉토리 추가 및 커스텀 모듈 임포트\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from multilayer_model.m3_hierarchical import HierarchicalCVAE3\n",
    "from multilayer_loss.l_multi3_hierarchical import l_multi3_hierarchical, l_multi3_bce_hierarchical\n",
    "from vae_earlystopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 3-Phase data loaded successfully.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 (사용자 지정 경로 및 파일명 반영)\n",
    "try:\n",
    "    x1_data = np.load('../data/metal.npy')             # Phase 1: Metal\n",
    "    x2_data = np.load('../data/support_norm.npy')      # Phase 2: Support\n",
    "    x3_data = np.load('../data/pre_fin.npy')      # Phase 3: Pretreatment\n",
    "    c_data = np.load('../data/re_fin.npy')             # Condition: Reaction/Active\n",
    "    print(\"✅ All 3-Phase data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "\n",
    "# 2. 데이터 분할 (Train/Val/Test) - 인덱스 일관성 유지\n",
    "indices = np.arange(len(x1_data))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "def split_and_scale(data, train_idx, val_idx, test_idx, scale=True):\n",
    "    train, val, test = data[train_idx], data[val_idx], data[test_idx]\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        train = scaler.fit_transform(train)\n",
    "        val = scaler.transform(val)\n",
    "        test = scaler.transform(test)\n",
    "        return train, val, test, scaler\n",
    "    return train, val, test, None\n",
    "\n",
    "x1_train, x1_val, x1_test, _ = split_and_scale(x1_data, train_idx, val_idx, test_idx)\n",
    "x2_train, x2_val, x2_test, _ = split_and_scale(x2_data, train_idx, val_idx, test_idx)\n",
    "x3_train, x3_val, x3_test, _ = split_and_scale(x3_data, train_idx, val_idx, test_idx)\n",
    "c_train, c_val, c_test, c_scaler = split_and_scale(c_data, train_idx, val_idx, test_idx)\n",
    "\n",
    "# 3. 텐서 변환 및 데이터로더\n",
    "def to_t(arr): return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(to_t(x1_train), to_t(x2_train), to_t(x3_train), to_t(c_train)), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(to_t(x1_val), to_t(x2_val), to_t(x3_val), to_t(c_val)), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(to_t(x1_test), to_t(x2_test), to_t(x3_test), to_t(c_test)), batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 3단계 계층 모델 초기화\n",
    "x_dims = [x1_train.shape[1], x2_train.shape[1], x3_train.shape[1]]\n",
    "c_dim = c_train.shape[1]\n",
    "z_dims = [16, 8, 4] # Metal -> Support -> Pretreatment 순으로 latent 압축\n",
    "\n",
    "model = HierarchicalCVAE3(x_dims, c_dim, z_dims).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "es = EarlyStopping(patience=50, min_delta=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 50\n",
      "Epoch 20/500 | V_Loss: 0.025761 | Anneal_W: 0.13\n",
      "EarlyStopping counter: 1 out of 50\n",
      "EarlyStopping counter: 2 out of 50\n",
      "EarlyStopping counter: 1 out of 50\n",
      "EarlyStopping counter: 2 out of 50\n",
      "EarlyStopping counter: 3 out of 50\n",
      "EarlyStopping counter: 4 out of 50\n",
      "EarlyStopping counter: 1 out of 50\n",
      "EarlyStopping counter: 2 out of 50\n",
      "EarlyStopping counter: 1 out of 50\n",
      "EarlyStopping counter: 2 out of 50\n",
      "EarlyStopping counter: 3 out of 50\n",
      "EarlyStopping counter: 4 out of 50\n",
      "EarlyStopping counter: 5 out of 50\n",
      "EarlyStopping counter: 6 out of 50\n",
      "EarlyStopping counter: 7 out of 50\n",
      "EarlyStopping counter: 8 out of 50\n",
      "EarlyStopping counter: 9 out of 50\n",
      "Epoch 40/500 | V_Loss: 0.022330 | Anneal_W: 0.27\n",
      "EarlyStopping counter: 10 out of 50\n",
      "EarlyStopping counter: 11 out of 50\n",
      "EarlyStopping counter: 12 out of 50\n",
      "EarlyStopping counter: 13 out of 50\n",
      "EarlyStopping counter: 14 out of 50\n",
      "EarlyStopping counter: 15 out of 50\n",
      "EarlyStopping counter: 16 out of 50\n",
      "EarlyStopping counter: 17 out of 50\n",
      "EarlyStopping counter: 18 out of 50\n",
      "EarlyStopping counter: 19 out of 50\n",
      "EarlyStopping counter: 20 out of 50\n",
      "EarlyStopping counter: 21 out of 50\n",
      "EarlyStopping counter: 22 out of 50\n",
      "EarlyStopping counter: 23 out of 50\n",
      "EarlyStopping counter: 24 out of 50\n",
      "EarlyStopping counter: 25 out of 50\n",
      "EarlyStopping counter: 26 out of 50\n",
      "EarlyStopping counter: 27 out of 50\n",
      "EarlyStopping counter: 28 out of 50\n",
      "EarlyStopping counter: 29 out of 50\n",
      "Epoch 60/500 | V_Loss: 0.026491 | Anneal_W: 0.40\n",
      "EarlyStopping counter: 30 out of 50\n",
      "EarlyStopping counter: 31 out of 50\n",
      "EarlyStopping counter: 32 out of 50\n",
      "EarlyStopping counter: 33 out of 50\n",
      "EarlyStopping counter: 34 out of 50\n",
      "EarlyStopping counter: 35 out of 50\n",
      "EarlyStopping counter: 36 out of 50\n",
      "EarlyStopping counter: 37 out of 50\n",
      "EarlyStopping counter: 38 out of 50\n",
      "EarlyStopping counter: 39 out of 50\n",
      "EarlyStopping counter: 40 out of 50\n",
      "EarlyStopping counter: 41 out of 50\n",
      "EarlyStopping counter: 42 out of 50\n",
      "EarlyStopping counter: 43 out of 50\n",
      "EarlyStopping counter: 44 out of 50\n",
      "EarlyStopping counter: 45 out of 50\n",
      "EarlyStopping counter: 46 out of 50\n",
      "EarlyStopping counter: 47 out of 50\n",
      "EarlyStopping counter: 48 out of 50\n",
      "EarlyStopping counter: 49 out of 50\n",
      "Epoch 80/500 | V_Loss: 0.030745 | Anneal_W: 0.53\n",
      "EarlyStopping counter: 50 out of 50\n",
      "Early stopping at epoch 80\n",
      "Restored best model with loss: 0.021733\n"
     ]
    }
   ],
   "source": [
    "# 5. 학습 루프 (KL Annealing 적용)\n",
    "epochs = 500\n",
    "total_anneal_steps = 150 # 150에폭 동안 KL 가중치를 서서히 증가\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for b1, b2, b3, bc in train_loader:\n",
    "        b1, b2, b3, bc = b1.to(device), b2.to(device), b3.to(device), bc.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(b1, b2, b3, bc)\n",
    "        \n",
    "        # 3단계 계층적 Loss 계산 (BCE for Metal, MSE for others)\n",
    "        # 여기서는 복합적인 학습을 위해 수치 복원(MSE) 위주로 구성된 l_multi3_hierarchical 사용\n",
    "        # 실제 금속 존재 여부가 중요하면 l_multi3_bce_hierarchical로 교체 가능\n",
    "        loss_results = l_multi3_hierarchical(\n",
    "            out['x2_hat'], b2, # 메인 타겟 예시 (Support)\n",
    "            out['mu_list'], out['lv_list'],\n",
    "            gamma_list=[0.1, 0.05, 0.02], # NVAE 기반 가중치 차등\n",
    "            step=epoch,\n",
    "            total_steps=total_anneal_steps\n",
    "        )\n",
    "        \n",
    "        loss = loss_results['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for v1, v2, v3, vc in val_loader:\n",
    "            v1, v2, v3, vc = v1.to(device), v2.to(device), v3.to(device), vc.to(device)\n",
    "            vout = model(v1, v2, v3, vc)\n",
    "            v_loss += l_multi3_hierarchical(\n",
    "                vout['x2_hat'], v2, vout['mu_list'], vout['lv_list'],\n",
    "                step=epoch, total_steps=total_anneal_steps\n",
    "            )['loss'].item()\n",
    "            \n",
    "    avg_v_loss = v_loss / len(val_loader)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} | V_Loss: {avg_v_loss:.6f} | Anneal_W: {loss_results['anneal_w']:.2f}\")\n",
    "    \n",
    "    if es(avg_v_loss, model):\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "es.load_best_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating New Catalyst Recipes ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# 테스트셋의 첫 번째 조건 사용\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     sample_c = \u001b[43mc_test\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m      7\u001b[39m     m_prob, s_hat, p_hat = model.sample(sample_c, num_samples=\u001b[32m1\u001b[39m, device=device)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated Metal Presence (Prob):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, m_prob.cpu().numpy())\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# 6. 생성 능력 검증 (Inference)\n",
    "print(\"\\n--- Generating New Catalyst Recipes ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 수정 부분: numpy 배열을 torch tensor로 변환 후 device로 이동\n",
    "    sample_c = torch.tensor(c_test[0:1], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # 모델의 sample 메서드 호출\n",
    "    m_prob, s_hat, p_hat = model.sample(sample_c, num_samples=1, device=device)\n",
    "    \n",
    "    print(\"Generated Metal Presence (Prob):\\n\", m_prob.cpu().numpy())\n",
    "    print(\"Generated Support Composition:\\n\", s_hat.cpu().numpy())\n",
    "    print(\"Generated Pretreatment Parameters:\\n\", p_hat.cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "torch.save(model.state_dict(), '../model/h_cvae_3phase_best.pth')\n",
    "print(\"\\n✅ Best model saved to model/h_cvae_3phase_best.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
